{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d9cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec14f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60941351",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_color(idx):\n",
    "    np.random.seed(idx)\n",
    "    return tuple(int(x) for x in np.random.randint(0, 255, 3))\n",
    "\n",
    "# Load MiDaS model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "midas = torch.hub.load(\"intel-isl/MiDaS\", \"MiDaS_small\")\n",
    "midas.to(device).eval()\n",
    "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\").small_transform\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO('best_mosaic.pt')\n",
    "\n",
    "# Load image\n",
    "frame = cv2.imread(r\"traffic_wala_dataset\\valid\\images\\test_mp4-13_jpg.rf.98cd77f75c4492f8f103aaf4ce2ca8f8.jpg\")\n",
    "if frame is None:\n",
    "    raise FileNotFoundError(\"Image not found.\")\n",
    "\n",
    "# Depth estimation\n",
    "input_batch = midas_transforms(frame).to(device)\n",
    "with torch.no_grad():\n",
    "    prediction = midas(input_batch)\n",
    "    depth_map = prediction.squeeze().cpu().numpy()\n",
    "\n",
    "# Detection\n",
    "results = model(frame)\n",
    "\n",
    "# Step 1: Extract vehicle centers\n",
    "vehicle_centers = []\n",
    "bbox_data = []  # Store extra info for drawing\n",
    "for box in results[0].boxes:\n",
    "    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "    cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "    vehicle_centers.append([cx, cy])\n",
    "    bbox_data.append((x1, y1, x2, y2, cx, cy))\n",
    "\n",
    "vehicle_centers = np.array(vehicle_centers)\n",
    "\n",
    "# Step 2: DBSCAN clustering\n",
    "if len(vehicle_centers) > 0:\n",
    "    clustering = DBSCAN(eps=100, min_samples=2).fit(vehicle_centers)\n",
    "    labels = clustering.labels_\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    colors = plt.cm.get_cmap('tab20', n_clusters if n_clusters > 0 else 1)\n",
    "\n",
    "    # Step 3: Draw cluster circles\n",
    "    for (cx, cy), label in zip(vehicle_centers, labels):\n",
    "        if label == -1:\n",
    "            color = (128, 128, 128)  # noise\n",
    "        else:\n",
    "            c = colors(label)\n",
    "            color = (int(c[2]*255), int(c[1]*255), int(c[0]*255))\n",
    "        cv2.circle(frame, (cx, cy), 10, color, -1)\n",
    "        cv2.putText(frame, f'{label}', (cx - 10, cy - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
    "\n",
    "# Step 4: Compute depth distances and proximity warnings\n",
    "vehicle_depths = {}\n",
    "too_close_pairs = set()\n",
    "for idx, (x1, y1, x2, y2, cx, cy) in enumerate(bbox_data):\n",
    "    cx = np.clip(cx, 0, depth_map.shape[1] - 1)\n",
    "    cy = np.clip(cy, 0, depth_map.shape[0] - 1)\n",
    "    depth_value = depth_map[cy, cx]\n",
    "    vehicle_depths[idx] = (cx, cy, depth_value)\n",
    "\n",
    "# Compare pairwise distances\n",
    "for i in range(len(vehicle_depths)):\n",
    "    for j in range(i + 1, len(vehicle_depths)):\n",
    "        (x1, y1, d1) = vehicle_depths[i]\n",
    "        (x2, y2, d2) = vehicle_depths[j]\n",
    "        pixel_dist = np.sqrt((x1 - x2) ** 2 + (y1 - y2) ** 2)\n",
    "        depth_diff = abs(d1 - d2)\n",
    "        combined_dist = pixel_dist + (depth_diff * 1000)\n",
    "        if combined_dist < 300:\n",
    "            too_close_pairs.add(i)\n",
    "            too_close_pairs.add(j)\n",
    "\n",
    "# Step 5: Draw final annotations\n",
    "for idx, (x1, y1, x2, y2, cx, cy) in enumerate(bbox_data):\n",
    "    color = get_color(idx)\n",
    "    cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "    cv2.putText(frame, f\"ID: {idx}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    if idx in too_close_pairs:\n",
    "        cv2.putText(frame, \"Too Close!\", (cx, cy - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "# Vehicle count\n",
    "cv2.putText(frame, f\"Total Vehicles: {len(bbox_data)}\", (20, 100),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "# Display\n",
    "cv2.imshow(\"Traffic Clustered\", frame)\n",
    "cv2.imwrite(\"output_image_result.jpg\", frame)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
